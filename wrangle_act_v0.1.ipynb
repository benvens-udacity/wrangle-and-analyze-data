{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project looks at various data sources for Tweets from the [WeRateDogs](https://twitter.com/dog_rates) Twitter account, specifically:\n",
    "\n",
    "1. the `twitter-archive-enhanced.csv` which contains the tweet text, as is the core data set\n",
    "1. the Twitter API is used to access the original tweets to retrieve missing fields such as the retweet and favorite counts\n",
    "1. an image prediction file containing the top 3 predictions for each of the (up to 4) dog pictures in the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATHER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a number of data assets including local files, remote files on web servers, and JSON payloads returned by the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WE_RATE_DOGS_TWEETS_PATH = 'data/twitter-archive-enhanced.csv'\n",
    "DOG_BREED_PREDICTIONS_SOURCE_URL = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `read_csv()` function is quite versatile when uploading data, and can be configured to handle different date formats, numeric data types, not available (NA) markers, etc. Getting this right upfront can save time, but requires the raw data in files to be eyeballed first. For this we can use command line tools like head & tail, or Excel, which allows column headings to be frozen, data to be sorted and searched, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'data/twitter-archive-enhanced.csv' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 11 {WE_RATE_DOGS_TWEETS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: cannot open 'data/twitter-archive-enhanced.csv' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1940 {WE_RATE_DOGS_TWEETS_PATH} | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the raw data, we make the following observations:\n",
    "\n",
    "1. tweet Ids are large integers, we need to select an approriate integer datatype so no accuracy is lost\n",
    "1. some tweet Ids use floats, e.g.: `in_reply_to_status_id`, `in_reply_to_user_id`, with NaNs used as a Not Available marker, as mentioned above these need to be converted to integers\n",
    "1. time stamps are close to ISO 8601 format, and are GMT \n",
    "1. some strings have embeded new line characters, `read_csv()` handles this, but it messes up commands like `wc -l`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions taken to address above observations:\n",
    "\n",
    "* convert floating point tweets Ids to a 64-bit integer, retaining the Not Available representation\n",
    "* specifcally tell Pandas which columns are dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-97b31d2ae964>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0myaml\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtweepy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the enhanced Twitter archive, using explicit data types for fields, instead of letting Pandas infer them. The [Twitter API](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object) will define the data types for the Twitter fields, to which I add the data types for the \"enhanced\" fields.\n",
    "\n",
    "To get around the fact that nullable numeric fields, by default, are interpreted by `read_csv()` as floats (so as to include NaN to represent null or Not Available), I am mapping optional tweet Ids to Pandas nullable integer data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data_types = {\n",
    "    'tweet_id': np.int64,\n",
    "    'in_reply_to_status_id': 'Int64',\n",
    "    'in_reply_to_user_id': 'Int64',\n",
    "    'retweeted_status_id': 'Int64',\n",
    "    'retweeted_status_user_id': 'Int64',\n",
    "    'text': 'string',\n",
    "    'expanded_urls': 'string',\n",
    "    'rating_numerator': np.int32,\n",
    "    'rating_denominator': np.int32,\n",
    "    'name': 'string',\n",
    "    'doggo': 'string',\n",
    "    'floofer': 'string',\n",
    "    'pupper': 'string',\n",
    "    'puppo': 'string'\n",
    "}\n",
    "\n",
    "feed_date_cols = [\n",
    "    'timestamp', \n",
    "    'retweeted_status_timestamp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(WE_RATE_DOGS_TWEETS_PATH,\n",
    "                        index_col=['tweet_id'],\n",
    "                        dtype=feed_data_types,\n",
    "                        parse_dates=feed_date_cols)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the core data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first discrepancy we note is that, according to the project motivation document, the main \"archive contains basic tweet data for all 5000+ of their tweets\" however that is clearly not the case as, having loaded it, the number of tweets is less than half that. As this is the master data set we have been provided with, this is the data we will go with, since it has been previously enhanced.\n",
    "\n",
    "Just to double check this row count, we will run a line count on the input file, which should roughly match the number of rows in the data frame. Any discrepancy on counts is due to those embeded new line (NL) characters in the tweet text, as was previously mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l {WE_RATE_DOGS_TWEETS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When displaying Pandas data frames in the Jupyter notebook, raise or remove any limits on number displayed\n",
    "# Care is needed here, specially with row counts, as very large data sets may get transferred into the browser\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can double check the column data types, against the data type mapping provided to `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Twitter API to enrich the core data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to use the Twitter API to retrieve the original tweets, so that we can enrich our enhanced tweets data with the missing attributes previously dientified (`retweet_counts`, `favorite_counts`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having registered with Twitter as a developer, and obtained credentials and keys, we stored these in a private project directory and configuration file (which are excluded from our git repo, and thus won't be visible online in [github](https://github.com/benvens-udacity/wrangle-and-analyze-data/blob/main/wrangle_act.ipynb)).\n",
    "\n",
    "We now use those credentials to authenticate with Twitter for API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_creds(conf_path):\n",
    "    with open(conf_path, 'r') as cf:\n",
    "        config = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = read_creds('./config/private/creds.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = creds['consumer_api']['key']\n",
    "consumer_secret = creds['consumer_api']['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = creds['access_token']['token']\n",
    "acess_secret = creds['access_token']['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, acess_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load the enrichment data in batches, for better performance, as API invocations are subject to significant network latency. Twitter also applies rate limiting to their APIs, so it is necessary to throttle the rate at which we make requests, and to retry any failed requests. Luckily, this can be handled automatically by the Tweepy library, by setting the `wait_on_rate_limit_notify` flag when configuring API connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are enriching the core enhanced tweets archive, we will initially load the API data into a separate data frame, cleanup as necessary, and then merge into the main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    idxs = []\n",
    "    retweet_counts = []\n",
    "    favorite_counts = []\n",
    "    for status in batch:\n",
    "        tweet = status._json\n",
    "        idxs.append(tweet['id'])\n",
    "        retweet_counts.append(tweet['retweet_count'])\n",
    "        favorite_counts.append(tweet['favorite_count'])\n",
    "    return np.array(idxs, dtype=np.int64), np.array([retweet_counts, favorite_counts], dtype=np.int64).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.empty((0), dtype=np.int64)\n",
    "rows = np.empty((0, 2), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_tweets = len(tweets_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for batch_start in range(0, num_tweets, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, num_tweets)\n",
    "    batch_tweet_ids = tweets_df.iloc[batch_start:batch_end].index.to_numpy().tolist()\n",
    "    statuses = api.statuses_lookup(batch_tweet_ids, include_entities=False, map_=False)\n",
    "    b_indices, b_rows = process_batch(statuses)\n",
    "    indices = np.concatenate((indices, b_indices), axis=0)\n",
    "    rows = np.concatenate((rows, b_rows), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts_df = pd.DataFrame(index=indices, data=rows, \n",
    "                               columns=['retweet_counts', 'favorite_counts'], \n",
    "                               dtype='Int32').sort_index()\n",
    "tweet_counts_df.index.name = 'tweet_id'\n",
    "tweet_counts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_tweets_df = tweets_df.merge(tweet_counts_df, how='inner', on='tweet_id')\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we review the structure of the merged data frame. In particular, the number of rows should be unchanged (as you'd expect given it is a left join), and the 2 additional columns have been added on at the end, with some NA values reflecting tweets that have never been retweeted or favorited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and normalise image data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to load the image predictions data, so we can tidy it. We will read this data from the CloudFront URL, a opposed to the local file, to ensure we get the most up-to-date version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds_data_types = {\n",
    "    'tweet_id': 'Int64',\n",
    "    'jpg_url': 'string',\n",
    "    'img_num': np.int32,\n",
    "    'p1': 'string',\n",
    "    'p1_conf': np.float32,\n",
    "    'p1_dog': bool,\n",
    "    'p2': 'string',\n",
    "    'p2_conf': np.float32,\n",
    "    'p2_dog': bool,\n",
    "    'p3': 'string',\n",
    "    'p3_conf': np.float32,\n",
    "    'p3_dog': bool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV (not CSV) records, and tell read_csv() to use a tab as the field separator\n",
    "\n",
    "img_preds_df = pd.read_csv(DOG_BREED_PREDICTIONS_SOURCE_URL,\n",
    "                           index_col=['tweet_id'],\n",
    "                           sep='\\t', \n",
    "                           dtype=img_preds_data_types)\n",
    "img_preds_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now briefly review the structure of this data frame: \n",
    "\n",
    "1. each row refers to an image\n",
    "1. each image is numbered, as it is selected as one of up to 4 dog images that may be associated with each tweet\n",
    "1. we then have the top 3 predictions for the image\n",
    "\n",
    "Each prediction consists of the following information:\n",
    "\n",
    "1. a predicted label or class (e.g.: the dog breed) tht describes that image\n",
    "1. a confidence factor associated with the previous prediction, in the range 0.0 -> 1.0\n",
    "1. a boolean indicator confirming if the predicted label is a dog breed, or some other object\n",
    "\n",
    "Looking at the confidence factors for predictions p1 - p3, they appear to be listed in most confident to least confident order. Therefore we will use the column name numeric suffix to generate a ranking column, which we can later sort by (to preserve this decreasing confidence order).\n",
    "\n",
    "This last attribute confirms that the image classifier used to generate these prediction was trained on a broad set of images, only a subset of which are dog images labelled with their corresponding dog breed. But on occasions the classifier may have interpreted a dog image as an object other than a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already assessed the core tweet data, and fixed some issues at load time. Lets now look at data quality and structural issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remedy data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the issues we now want to remedy are: \n",
    "\n",
    "1. dog names and stages are extracted when found, otherwise the value _None_ is used\n",
    "1. dog stages are predefined, so storing the stage name into the relevant stage columnis redundant information, all we require is a binary marker\n",
    "1. the 'source' column is an HTML anchor with a link to http://twitter.com/download/iphone which is repeated for all rows, therefore we will drop the `source` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So next we convert those dog stage columns into a boolean data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dog stage columns into a boolean data type\n",
    "\n",
    "stage_cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "enriched_tweets_df[stage_cols] = enriched_tweets_df[stage_cols].apply(lambda c: c.to_numpy() != 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the relevant dog stage columns are booleans (not strings)\n",
    "\n",
    "enriched_tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop uninformative 'source' column\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns=['source'])\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remedy structural issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `expanded_urls` column, which stores the full length URL for shortened URLs that appear in the tweet, has a couple of issues, both quality and structure:\n",
    "\n",
    "1. it can store multiple URLs per cell, as a comma separated string\n",
    "1. the same URL can appear multiple times\n",
    "\n",
    "As looking at the tweet text it is not obvious why the same URL can appear more than once, instead of storing a repetition count, we will just drop any duplicates. Since these URLs can now have a many-to-one relationship with the tweet in which they appear, we will store them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out rows containing one or more expanded URLs, as some rows have none\n",
    "\n",
    "expanded_urls_ser = enriched_tweets_df.loc[enriched_tweets_df['expanded_urls'].isna() == False]['expanded_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested list comprehension to split multiple URL strings on comma separator, then create [tweet Id, URL] tuples\n",
    "\n",
    "expanded_url_tuples = [(ix, url) for ix, urls in expanded_urls_ser.iteritems() for url in urls.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_url_df = pd.DataFrame(expanded_url_tuples, columns=['tweet_id', 'expanded_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop duplicates and make 'tweet_id' the index for consistency with other data frames\n",
    "\n",
    "expanded_url_df = expanded_url_df.drop_duplicates().set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally drop the original expanded_urls column\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns='expanded_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the index can contain duplicate entries (where a tweet has more than one URL)\n",
    "# We compare duplicate and non-duplicate counts below\n",
    "\n",
    "expanded_url_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(expanded_url_df.index), len(expanded_url_df.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will tidy up the image predictions data. By applying the Tidy Data principles, we will remove variables (the prediction number) from the relevant column names. \n",
    "\n",
    "We are repeating the same image URL and image number, on each of the predictions, just for simplicity (the alternative is to split the image data into 2 data frames). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_pred_cols(df, numeric):\n",
    "    preds_df = df[['jpg_url', 'img_num']]\n",
    "    preds_df = preds_df.assign(pred_rank=numeric,\n",
    "                               pred_class=img_preds_df[f'p{numeric}'],\n",
    "                               pred_confidence=img_preds_df[f'p{numeric}_conf'],\n",
    "                               pred_is_dog=img_preds_df[f'p{numeric}_dog'])\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_preds_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1_df = slice_pred_cols(img_preds_df, 1)\n",
    "preds2_df = slice_pred_cols(img_preds_df, 2)\n",
    "preds3_df = slice_pred_cols(img_preds_df, 3)\n",
    "image_predictions_df = pd.concat([preds1_df, preds2_df, preds3_df]).sort_values(by=['tweet_id', 'pred_rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are removing variables from the column names, we will end up with more rows. Since all top 3 predictions are always generated, we will have exactly 3 times the number of rows we started with, as reflected by the row counts above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already performed a few cleaning tasks, including correcting data types and eliminating redundant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address specific data cleaning requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will clean up the data as specified under the section **Key Points**, in the Project Motivation page, specifically:\n",
    "\n",
    "1. We drop retweets, [by definition](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object) these are tweets where the `retweet_status` fields are populated\n",
    "2. Since none of the remaining rows are retweets, any column related to retweets will only hold NA and are now redundant, so we drop these columns\n",
    "3. We drop columns without images, i.e.: where the `tweet_id` does not appear in the images data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete retweets based on the presence of a retweet time, the drop all redundant retweet related columns\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.loc[enriched_tweets_df['retweeted_status_timestamp'].isna()]\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns=['retweeted_status_id', \n",
    "                                                      'retweeted_status_user_id', \n",
    "                                                      'retweeted_status_timestamp'])\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete tweets without images\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.loc[enriched_tweets_df.index.intersection(img_preds_df.index, sort=None)]\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep persistent copy of wrangled clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there appear to be no tweets beyond August 1st, 2017 (most likely since we dropped tweets without images) we are now done with cleaning. We end the wranggling process by writing the clean data to a new set of clean data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now write the cleaned data frames into files, in a directory called 'clean'\n",
    "\n",
    "enriched_tweets_df.to_csv('clean/twitter_archive_master.csv')\n",
    "expanded_url_df.to_csv('clean/twitter_archive_urls.csv')\n",
    "image_predictions_df.to_csv('clean/image_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data insights and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at the data and query it to obtain some insights. Specifically, we are interested in:\n",
    "\n",
    "1. Finding the number of tweets with a score above 10/10, versus tweets with a score under 10/10\n",
    "2. Identify the tweets where more than one dog stage appears\n",
    "3. Finding the number of top breed predictions from the image classifier, with a predictionconfidence below 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count number of scores above and below 10/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((enriched_tweets_df['rating_numerator'] / enriched_tweets_df['rating_denominator']) > 1.0).sum(), \\\n",
    "((enriched_tweets_df['rating_numerator'] / enriched_tweets_df['rating_denominator']) <= 1.0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show tweets with more than one dog stage in the tweet text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "enriched_tweets_df.loc[enriched_tweets_df[stage_cols].sum(axis=1) > 1][['text'] + stage_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count tweets where the top scoring breed prediction is below 0.5** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_preds = image_predictions_df.loc[(image_predictions_df['pred_rank'] == 1) \\\n",
    "                                     & image_predictions_df['pred_is_dog']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dog_preds[dog_preds['pred_confidence'] < 0.5].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to generate some visualisations:\n",
    "\n",
    "1. First, based on the top image prediction, look at the frequency distribution for the top 10 breeds only, based on number of tweets\n",
    "2. Now look at the frequency distribution for the top 10 breeds only, based on aggregate number of favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution by number of tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_preds['pred_class'].value_counts(sort=True)[0:10].plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution by number of favorites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_preds.join(enriched_tweets_df['favorite_counts']).groupby(['pred_class']) \\\n",
    "    .sum().sort_values(by='favorite_counts', ascending=False)[0:10]['favorite_counts'].plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate internal report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cleaned the data, and generated data insights, we can now generate the internal documentation from this notebook's markdown cells.\n",
    "\n",
    "(you probably want to clear all output previous to the data insights output generated in the last section, and then SAVE the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --no-input --to pdf wrangle_act.ipynb\n",
    "!mv wrangle_act.pdf wrangle_report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will build a quick image classifier to try and predict dog breeds, and compare against the existing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}