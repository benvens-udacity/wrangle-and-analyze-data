{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project looks at various data sources for Tweets from the [WeRateDogs](https://twitter.com/dog_rates) Twitter account, specifically:\n",
    "\n",
    "1. the `twitter-archive-enhanced.csv` which contains the tweet text, as is the core data set\n",
    "1. the Twitter API is used to access the original tweets to retrieve missing fields such as the retweet and favorite counts\n",
    "1. an image prediction file containing the top 3 predictions for each of the (up to 4) dog pictures in the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATHER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a number of data assets including local files, remote files on web servers, and JSON payloads returned by the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image-predictions.tsv        tweet-json.zip\r\n",
      "tweet-json copy              twitter-archive-enhanced.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WE_RATE_DOGS_TWEETS_PATH = 'data/twitter-archive-enhanced.csv'\n",
    "DOG_BREED_PREDICTIONS_SOURCE_URL = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `read_csv()` function is quite versatile when uploading data, and can be configured to handle different date formats, numeric data types, not available (NA) markers, etc. Getting this right upfront can save time, but requires the raw data in files to be eyeballed first. For this we can use command line tools like head & tail, or Excel, which allows column headings to be frozen, data to be sorted and searched, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id,in_reply_to_status_id,in_reply_to_user_id,timestamp,source,text,retweeted_status_id,retweeted_status_user_id,retweeted_status_timestamp,expanded_urls,rating_numerator,rating_denominator,name,doggo,floofer,pupper,puppo\r\n",
      "892420643555336193,,,2017-08-01 16:23:56 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Phineas. He's a mystical boy. Only ever appears in the hole of a donut. 13/10 https://t.co/MgUWQ76dJU,,,,https://twitter.com/dog_rates/status/892420643555336193/photo/1,13,10,Phineas,None,None,None,None\r\n",
      "892177421306343426,,,2017-08-01 00:17:27 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",\"This is Tilly. She's just checking pup on you. Hopes you're doing ok. If not, she's available for pats, snugs, boops, the whole bit. 13/10 https://t.co/0Xxu71qeIV\",,,,https://twitter.com/dog_rates/status/892177421306343426/photo/1,13,10,Tilly,None,None,None,None\r\n",
      "891815181378084864,,,2017-07-31 00:18:03 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Archie. He is a rare Norwegian Pouncing Corgo. Lives in the tall grass. You never know when one may strike. 12/10 https://t.co/wUnZnhtVJB,,,,https://twitter.com/dog_rates/status/891815181378084864/photo/1,12,10,Archie,None,None,None,None\r\n",
      "891689557279858688,,,2017-07-30 15:58:51 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Darla. She commenced a snooze mid meal. 13/10 happens to the best of us https://t.co/tD36da7qLQ,,,,https://twitter.com/dog_rates/status/891689557279858688/photo/1,13,10,Darla,None,None,None,None\r\n",
      "891327558926688256,,,2017-07-29 16:00:24 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",\"This is Franklin. He would like you to stop calling him \"\"cute.\"\" He is a very fierce shark and should be respected as such. 12/10 #BarkWeek https://t.co/AtUZn91f7f\",,,,\"https://twitter.com/dog_rates/status/891327558926688256/photo/1,https://twitter.com/dog_rates/status/891327558926688256/photo/1\",12,10,Franklin,None,None,None,None\r\n",
      "891087950875897856,,,2017-07-29 00:08:17 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",Here we have a majestic great white breaching off South Africa's coast. Absolutely h*ckin breathtaking. 13/10 (IG: tucker_marlo) #BarkWeek https://t.co/kQ04fDDRmh,,,,https://twitter.com/dog_rates/status/891087950875897856/photo/1,13,10,None,None,None,None,None\r\n",
      "890971913173991426,,,2017-07-28 16:27:12 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",\"Meet Jax. He enjoys ice cream so much he gets nervous around it. 13/10 help Jax enjoy more things by clicking below\r\n",
      "\r\n",
      "https://t.co/Zr4hWfAs1H https://t.co/tVJBRMnhxl\",,,,\"https://gofundme.com/ydvmve-surgery-for-jax,https://twitter.com/dog_rates/status/890971913173991426/photo/1\",13,10,Jax,None,None,None,None\r\n",
      "890729181411237888,,,2017-07-28 00:22:40 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",When you watch your owner call another dog a good boy but then they turn back to you and say you're a great boy. 13/10 https://t.co/v0nONBcwxq,,,,\"https://twitter.com/dog_rates/status/890729181411237888/photo/1,https://twitter.com/dog_rates/status/890729181411237888/photo/1\",13,10,None,None,None,None,None\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 11 {WE_RATE_DOGS_TWEETS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813142292504645637,,,2016-12-25 22:00:04 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",Everybody stop what you're doing and look at this dog with her tiny Santa hat. 13/10 https://t.co/KK4XQK9SPi,,,,\"https://twitter.com/dog_rates/status/813142292504645637/photo/1,https://twitter.com/dog_rates/status/813142292504645637/photo/1,https://twitter.com/dog_rates/status/813142292504645637/photo/1\",13,10,None,None,None,None,None\n",
      "813130366689148928,8.13127251579564e+17,4196983835.0,2016-12-25 21:12:41 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",I've been informed by multiple sources that this is actually a dog elf who's tired from helping Santa all night. Pupgraded to 12/10,,,,,12,10,None,None,None,None,None\n",
      "813127251579564032,,,2016-12-25 21:00:18 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",Here's an anonymous doggo that appears to be very done with Christmas. 11/10 cheer up pup https://t.co/BzITyGw3JA,,,,\"https://twitter.com/dog_rates/status/813127251579564032/photo/1,https://twitter.com/dog_rates/status/813127251579564032/photo/1\",11,10,None,doggo,None,None,None\n",
      "813112105746448384,,,2016-12-25 20:00:07 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",Meet Toby. He's pupset because his hat isn't big enough. Christmas is ruined. 12/10 it'll be ok Toby https://t.co/zfdaGZlweq,,,,https://twitter.com/dog_rates/status/813112105746448384/photo/1,12,10,Toby,None,None,None,None\n",
      "813096984823349248,,,2016-12-25 19:00:02 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Rocky. He got triple-doggo-dared. Stuck af. 11/10 someone help him https://t.co/soNL00XWVu,,,,https://twitter.com/dog_rates/status/813096984823349248/photo/1,11,10,Rocky,doggo,None,None,None\n",
      "813081950185472002,,,2016-12-25 18:00:17 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Baron. He's officially festive as h*ck. Thinks it's just a fancy scarf. 11/10 would pat head approvingly https://t.co/PjulYEXTvg,,,,\"https://twitter.com/dog_rates/status/813081950185472002/photo/1,https://twitter.com/dog_rates/status/813081950185472002/photo/1\",11,10,Baron,None,None,None,None\n",
      "813066809284972545,,,2016-12-25 17:00:08 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Tyr. He is disgusted by holiday traffic. Just trying to get to Christmas brunch on time. 12/10 hurry up pup https://t.co/syuTXARdtN,,,,https://twitter.com/dog_rates/status/813066809284972545/photo/1,12,10,Tyr,None,None,None,None\n",
      "813051746834595840,,,2016-12-25 16:00:16 +0000,\"<a href=\"\"http://twitter.com/download/iphone\"\" rel=\"\"nofollow\"\">Twitter for iPhone</a>\",This is Bauer. He had nothing to do with the cookies that disappeared. 13/10 very good boy https://t.co/AIMF8ouzvl,,,,https://twitter.com/dog_rates/status/813051746834595840/photo/1,13,10,Bauer,None,None,None,None\n",
      "tail: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1940 {WE_RATE_DOGS_TWEETS_PATH} | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the raw data, we make the following observations:\n",
    "\n",
    "1. tweet Ids are large integers, we need to select an approriate integer datatype so no accuracy is lost\n",
    "1. some tweet Ids use floats, e.g.: `in_reply_to_status_id`, `in_reply_to_user_id`, with NaNs used as a Not Available marker, as mentioned above these need to be converted to integers\n",
    "1. time stamps are close to ISO 8601 format, and are GMT \n",
    "1. some strings have embeded new line characters, `read_csv()` handles this, but it messes up commands like `wc -l`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions taken to address above observations:\n",
    "\n",
    "* convert floating point tweets Ids to a 64-bit integer, retaining the Not Available representation\n",
    "* specifcally tell Pandas which columns are dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the enhanced Twitter archive, using explicit data types for fields, instead of letting Pandas infer them. The [Twitter API](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object) will define the data types for the Twitter fields, to which I add the data types for the \"enhanced\" fields.\n",
    "\n",
    "To get around the fact that nullable numeric fields, by default, are interpreted by `read_csv()` as floats (so as to include NaN to represent null or Not Available), I am mapping optional tweet Ids to Pandas nullable integer data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data_types = {\n",
    "    'tweet_id': np.int64,\n",
    "    'in_reply_to_status_id': 'Int64',\n",
    "    'in_reply_to_user_id': 'Int64',\n",
    "    'retweeted_status_id': 'Int64',\n",
    "    'retweeted_status_user_id': 'Int64',\n",
    "    'text': 'string',\n",
    "    'expanded_urls': 'string',\n",
    "    'rating_numerator': np.int32,\n",
    "    'rating_denominator': np.int32,\n",
    "    'name': 'string',\n",
    "    'doggo': 'string',\n",
    "    'floofer': 'string',\n",
    "    'pupper': 'string',\n",
    "    'puppo': 'string'\n",
    "}\n",
    "\n",
    "feed_date_cols = [\n",
    "    'timestamp', \n",
    "    'retweeted_status_timestamp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(WE_RATE_DOGS_TWEETS_PATH,\n",
    "                        index_col=['tweet_id'],\n",
    "                        dtype=feed_data_types,\n",
    "                        parse_dates=feed_date_cols)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the core data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first discrepancy we note is that, according to the project motivation document, the main \"archive contains basic tweet data for all 5000+ of their tweets\" however that is clearly not the case as, having loaded it, the number of tweets is less than half that. As this is the master data set we have been provided with, this is the data we will go with, since it has been previously enhanced.\n",
    "\n",
    "Just to double check this row count, we will run a line count on the input file, which should roughly match the number of rows in the data frame. Any discrepancy on counts is due to those embeded new line (NL) characters in the tweet text, as was previously mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2518 data/twitter-archive-enhanced.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {WE_RATE_DOGS_TWEETS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When displaying Pandas data frames in the Jupyter notebook, raise or remove any limits on number displayed\n",
    "# Care is needed here, specially with row counts, as very large data sets may get transferred into the browser\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can double check the column data types, against the data type mapping provided to `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2356 entries, 892420643555336193 to 666020888022790149\n",
      "Data columns (total 16 columns):\n",
      " #   Column                      Non-Null Count  Dtype              \n",
      "---  ------                      --------------  -----              \n",
      " 0   in_reply_to_status_id       78 non-null     Int64              \n",
      " 1   in_reply_to_user_id         78 non-null     Int64              \n",
      " 2   timestamp                   2356 non-null   datetime64[ns, UTC]\n",
      " 3   source                      2356 non-null   object             \n",
      " 4   text                        2356 non-null   string             \n",
      " 5   retweeted_status_id         181 non-null    Int64              \n",
      " 6   retweeted_status_user_id    181 non-null    Int64              \n",
      " 7   retweeted_status_timestamp  181 non-null    datetime64[ns, UTC]\n",
      " 8   expanded_urls               2297 non-null   string             \n",
      " 9   rating_numerator            2356 non-null   int32              \n",
      " 10  rating_denominator          2356 non-null   int32              \n",
      " 11  name                        2356 non-null   string             \n",
      " 12  doggo                       2356 non-null   string             \n",
      " 13  floofer                     2356 non-null   string             \n",
      " 14  pupper                      2356 non-null   string             \n",
      " 15  puppo                       2356 non-null   string             \n",
      "dtypes: Int64(4), datetime64[ns, UTC](2), int32(2), object(1), string(7)\n",
      "memory usage: 303.7+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Twitter API to enrich the core data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to use the Twitter API to retrieve the original tweets, so that we can enrich our enhanced tweets data with the missing attributes previously dientified (`retweet_counts`, `favorite_counts`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having registered with Twitter as a developer, and obtained credentials and keys, we stored these in a private project directory and configuration file (which are excluded from our git repo, and thus won't be visible online in [github](https://github.com/benvens-udacity/wrangle-and-analyze-data/blob/main/wrangle_act.ipynb)).\n",
    "\n",
    "We now use those credentials to authenticate with Twitter for API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_creds(conf_path):\n",
    "    with open(conf_path, 'r') as cf:\n",
    "        config = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = read_creds('./config/private/creds.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = creds['consumer_api']['key']\n",
    "consumer_secret = creds['consumer_api']['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = creds['access_token']['token']\n",
    "acess_secret = creds['access_token']['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, acess_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load the enrichment data in batches, for better performance, as API invocations are subject to significant network latency. Twitter also applies rate limiting to their APIs, so it is necessary to throttle the rate at which we make requests, and to retry any failed requests. Luckily, this can be handled automatically by the Tweepy library, by setting the `wait_on_rate_limit_notify` flag when configuring API connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are enriching the core enhanced tweets archive, we will initially load the API data into a separate data frame, cleanup as necessary, and then merge into the main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    idxs = []\n",
    "    retweet_counts = []\n",
    "    favorite_counts = []\n",
    "    for status in batch:\n",
    "        tweet = status._json\n",
    "        idxs.append(tweet['id'])\n",
    "        retweet_counts.append(tweet['retweet_count'])\n",
    "        favorite_counts.append(tweet['favorite_count'])\n",
    "    return np.array(idxs, dtype=np.int64), np.array([retweet_counts, favorite_counts], dtype=np.int64).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.empty((0), dtype=np.int64)\n",
    "rows = np.empty((0, 2), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_tweets = len(tweets_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 817 ms, sys: 110 ms, total: 926 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for batch_start in range(0, num_tweets, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, num_tweets)\n",
    "    batch_tweet_ids = tweets_df.iloc[batch_start:batch_end].index.to_numpy().tolist()\n",
    "    statuses = api.statuses_lookup(batch_tweet_ids, include_entities=False, map_=False)\n",
    "    b_indices, b_rows = process_batch(statuses)\n",
    "    indices = np.concatenate((indices, b_indices), axis=0)\n",
    "    rows = np.concatenate((rows, b_rows), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2331, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_counts_df = pd.DataFrame(index=indices, data=rows, \n",
    "                               columns=['retweet_counts', 'favorite_counts'], \n",
    "                               dtype='Int32').sort_index()\n",
    "tweet_counts_df.index.name = 'tweet_id'\n",
    "tweet_counts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2331, 18)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_tweets_df = tweets_df.merge(tweet_counts_df, how='inner', on='tweet_id')\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we review the structure of the merged data frame. In particular, the number of rows should be unchanged (as you'd expect given it is a left join), and the 2 additional columns have been added on at the end, with some NA values reflecting tweets that have never been retweeted or favorited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2331 entries, 892420643555336193 to 666020888022790149\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count  Dtype              \n",
      "---  ------                      --------------  -----              \n",
      " 0   in_reply_to_status_id       78 non-null     Int64              \n",
      " 1   in_reply_to_user_id         78 non-null     Int64              \n",
      " 2   timestamp                   2331 non-null   datetime64[ns, UTC]\n",
      " 3   source                      2331 non-null   object             \n",
      " 4   text                        2331 non-null   string             \n",
      " 5   retweeted_status_id         163 non-null    Int64              \n",
      " 6   retweeted_status_user_id    163 non-null    Int64              \n",
      " 7   retweeted_status_timestamp  163 non-null    datetime64[ns, UTC]\n",
      " 8   expanded_urls               2272 non-null   string             \n",
      " 9   rating_numerator            2331 non-null   int32              \n",
      " 10  rating_denominator          2331 non-null   int32              \n",
      " 11  name                        2331 non-null   string             \n",
      " 12  doggo                       2331 non-null   string             \n",
      " 13  floofer                     2331 non-null   string             \n",
      " 14  pupper                      2331 non-null   string             \n",
      " 15  puppo                       2331 non-null   string             \n",
      " 16  retweet_counts              2331 non-null   Int32              \n",
      " 17  favorite_counts             2331 non-null   Int32              \n",
      "dtypes: Int32(2), Int64(4), datetime64[ns, UTC](2), int32(2), object(1), string(7)\n",
      "memory usage: 323.2+ KB\n"
     ]
    }
   ],
   "source": [
    "enriched_tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and normalise image data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to load the image predictions data, so we can tidy it. We will read this data from the CloudFront URL, a opposed to the local file, to ensure we get the most up-to-date version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds_data_types = {\n",
    "    'tweet_id': 'Int64',\n",
    "    'jpg_url': 'string',\n",
    "    'img_num': np.int32,\n",
    "    'p1': 'string',\n",
    "    'p1_conf': np.float32,\n",
    "    'p1_dog': bool,\n",
    "    'p2': 'string',\n",
    "    'p2_conf': np.float32,\n",
    "    'p2_dog': bool,\n",
    "    'p3': 'string',\n",
    "    'p3_conf': np.float32,\n",
    "    'p3_dog': bool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2075, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the TSV (not CSV) records, and tell read_csv() to use a tab as the field separator\n",
    "\n",
    "img_preds_df = pd.read_csv(DOG_BREED_PREDICTIONS_SOURCE_URL,\n",
    "                           index_col=['tweet_id'],\n",
    "                           sep='\\t', \n",
    "                           dtype=img_preds_data_types)\n",
    "img_preds_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now briefly review the structure of this data frame: \n",
    "\n",
    "1. each row refers to an image\n",
    "1. each image is numbered, as it is selected as one of up to 4 dog images that may be associated with each tweet\n",
    "1. we then have the top 3 predictions for the image\n",
    "\n",
    "Each prediction consists of the following information:\n",
    "\n",
    "1. a predicted label or class (e.g.: the dog breed) tht describes that image\n",
    "1. a confidence factor associated with the previous prediction, in the range 0.0 -> 1.0\n",
    "1. a boolean indicator confirming if the predicted label is a dog breed, or some other object\n",
    "\n",
    "Looking at the confidence factors for predictions p1 - p3, they appear to be listed in most confident to least confident order. Therefore we will use the column name numeric suffix to generate a ranking column, which we can later sort by (to preserve this decreasing confidence order).\n",
    "\n",
    "This last attribute confirms that the image classifier used to generate these prediction was trained on a broad set of images, only a subset of which are dog images labelled with their corresponding dog breed. But on occasions the classifier may have interpreted a dog image as an object other than a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2075 entries, 666020888022790149 to 892420643555336193\n",
      "Data columns (total 11 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   jpg_url  2075 non-null   string \n",
      " 1   img_num  2075 non-null   int32  \n",
      " 2   p1       2075 non-null   string \n",
      " 3   p1_conf  2075 non-null   float32\n",
      " 4   p1_dog   2075 non-null   bool   \n",
      " 5   p2       2075 non-null   string \n",
      " 6   p2_conf  2075 non-null   float32\n",
      " 7   p2_dog   2075 non-null   bool   \n",
      " 8   p3       2075 non-null   string \n",
      " 9   p3_conf  2075 non-null   float32\n",
      " 10  p3_dog   2075 non-null   bool   \n",
      "dtypes: bool(3), float32(3), int32(1), string(4)\n",
      "memory usage: 119.6+ KB\n"
     ]
    }
   ],
   "source": [
    "img_preds_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already assessed the core tweet data, and fixed some issues at load time. Lets now look at data quality and structural issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remedy data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the issues we now want to remedy are: \n",
    "\n",
    "1. dog names and stages are extracted when found, otherwise the value _None_ is used\n",
    "1. dog stages are predefined, so storing the stage name into the relevant stage columnis redundant information, all we require is a binary marker\n",
    "1. the 'source' column is an HTML anchor with a link to http://twitter.com/download/iphone which is repeated for all rows, therefore we will drop the `source` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So next we convert those dog stage columns into a boolean data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dog stage columns into a boolean data type\n",
    "\n",
    "stage_cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "enriched_tweets_df[stage_cols] = enriched_tweets_df[stage_cols].apply(lambda c: c.to_numpy() != 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2331 entries, 892420643555336193 to 666020888022790149\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count  Dtype              \n",
      "---  ------                      --------------  -----              \n",
      " 0   in_reply_to_status_id       78 non-null     Int64              \n",
      " 1   in_reply_to_user_id         78 non-null     Int64              \n",
      " 2   timestamp                   2331 non-null   datetime64[ns, UTC]\n",
      " 3   source                      2331 non-null   object             \n",
      " 4   text                        2331 non-null   string             \n",
      " 5   retweeted_status_id         163 non-null    Int64              \n",
      " 6   retweeted_status_user_id    163 non-null    Int64              \n",
      " 7   retweeted_status_timestamp  163 non-null    datetime64[ns, UTC]\n",
      " 8   expanded_urls               2272 non-null   string             \n",
      " 9   rating_numerator            2331 non-null   int32              \n",
      " 10  rating_denominator          2331 non-null   int32              \n",
      " 11  name                        2331 non-null   string             \n",
      " 12  doggo                       2331 non-null   bool               \n",
      " 13  floofer                     2331 non-null   bool               \n",
      " 14  pupper                      2331 non-null   bool               \n",
      " 15  puppo                       2331 non-null   bool               \n",
      " 16  retweet_counts              2331 non-null   Int32              \n",
      " 17  favorite_counts             2331 non-null   Int32              \n",
      "dtypes: Int32(2), Int64(4), bool(4), datetime64[ns, UTC](2), int32(2), object(1), string(3)\n",
      "memory usage: 259.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the relevant dog stage columns are booleans (not strings)\n",
    "\n",
    "enriched_tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2331, 17)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop uninformative 'source' column\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns=['source'])\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remedy structural issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `expanded_urls` column, which stores the full length URL for shortened URLs that appear in the tweet, has a couple of issues, both quality and structure:\n",
    "\n",
    "1. it can store multiple URLs per cell, as a comma separated string\n",
    "1. the same URL can appear multiple times\n",
    "\n",
    "As looking at the tweet text it is not obvious why the same URL can appear more than once, instead of storing a repetition count, we will just drop any duplicates. Since these URLs can now have a many-to-one relationship with the tweet in which they appear, we will store them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out rows containing one or more expanded URLs, as some rows have none\n",
    "\n",
    "expanded_urls_ser = enriched_tweets_df.loc[enriched_tweets_df['expanded_urls'].isna() == False]['expanded_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested list comprehension to split multiple URL strings on comma separator, then create [tweet Id, URL] tuples\n",
    "\n",
    "expanded_url_tuples = [(ix, url) for ix, urls in expanded_urls_ser.iteritems() for url in urls.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_url_df = pd.DataFrame(expanded_url_tuples, columns=['tweet_id', 'expanded_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop duplicates and make 'tweet_id' the index for consistency with other data frames\n",
    "\n",
    "expanded_url_df = expanded_url_df.drop_duplicates().set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally drop the original expanded_urls column\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns='expanded_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2313 entries, 892420643555336193 to 666020888022790149\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   expanded_url  2313 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Note that the index can contain duplicate entries (where a tweet has more than one URL)\n",
    "# We compare duplicate and non-duplicate counts below\n",
    "\n",
    "expanded_url_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 2272)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_url_df.index), len(expanded_url_df.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will tidy up the image predictions data. By applying the Tidy Data principles, we will remove variables (the prediction number) from the relevant column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_pred_cols(df, numeric):\n",
    "    preds_df = df[['jpg_url', 'img_num']]\n",
    "    preds_df = preds_df.assign(pred_rank=numeric,\n",
    "                               pred_class=img_preds_df[f'p{numeric}'],\n",
    "                               pred_confidence=img_preds_df[f'p{numeric}_conf'],\n",
    "                               pred_is_dog=img_preds_df[f'p{numeric}_dog'])\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2075"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_preds_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1_df = slice_pred_cols(img_preds_df, 1)\n",
    "preds2_df = slice_pred_cols(img_preds_df, 2)\n",
    "preds3_df = slice_pred_cols(img_preds_df, 3)\n",
    "image_predictions_df = pd.concat([preds1_df, preds2_df, preds3_df]).sort_values(by=['tweet_id', 'pred_rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are removing variables from the column names, we will end up with more rows. Since all top 3 predictions are always generated, we will have exactly 3 times the number of rows we started with, as reflected by the row counts above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6225 entries, 666020888022790149 to 892420643555336193\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   jpg_url          6225 non-null   string \n",
      " 1   img_num          6225 non-null   int32  \n",
      " 2   pred_rank        6225 non-null   int64  \n",
      " 3   pred_class       6225 non-null   string \n",
      " 4   pred_confidence  6225 non-null   float32\n",
      " 5   pred_is_dog      6225 non-null   bool   \n",
      "dtypes: bool(1), float32(1), int32(1), int64(1), string(2)\n",
      "memory usage: 249.2 KB\n"
     ]
    }
   ],
   "source": [
    "image_predictions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already performed a few cleaning tasks, including correcting data types and eliminating redundant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address specific data cleaning requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will clean up the data as specified under the section **Key Points**, in the Project Motivation page, specifically:\n",
    "\n",
    "1. we drop retweets, [by definition](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object) these are tweets where the `retweet_status` fields are populated\n",
    "2. since none of the remaining rows are retweets, any column related to retweets will only hold NA and are now redundant, so we drop these columns\n",
    "3. we drop columns without images, i.e.: where the `tweet_id` does not appear in the images data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete retweets based on the presence of a retweet time, the drop all redundant retweet related columns\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.loc[enriched_tweets_df['retweeted_status_timestamp'].isna()]\n",
    "enriched_tweets_df = enriched_tweets_df.drop(columns=['retweeted_status_id', \n",
    "                                                      'retweeted_status_user_id', \n",
    "                                                      'retweeted_status_timestamp'])\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete tweets without images\n",
    "\n",
    "enriched_tweets_df = enriched_tweets_df.loc[enriched_tweets_df.index.intersection(img_preds_df.index, sort=None)]\n",
    "enriched_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep persistent copy of wranggled clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there appear to be no tweets beyond August 1st, 2017 (most likely since we dropped tweets without images) we are now done with cleaning. We end the wranggling process by writing the clean data to a new set of clean data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now write the cleaned data frames into files, in a directory called 'clean'\n",
    "\n",
    "enriched_tweets_df.to_csv('clean/twitter_archive_master.csv')\n",
    "expanded_url_df.to_csv('clean/twitter_archive_urls.csv')\n",
    "image_predictions_df.to_csv('clean/image_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate internal report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cleaned the data, we can now generate the internal documentation from this notebook's markdown cells.\n",
    "\n",
    "(you probably want to clear all output before running the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook wrangle_act.ipynb to pdf\n",
      "[NbConvertApp] Writing 47329 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 56891 bytes to wrangle_act.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --no-input --to pdf wrangle_act.ipynb\n",
    "!mv wrangle_act.pdf wrangle_report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
